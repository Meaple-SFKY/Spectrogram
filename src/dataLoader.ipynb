{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizer\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io as scio\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 19 19 19]\n",
      "7032 7032\n",
      "torch.Size([1, 369, 148])\n"
     ]
    }
   ],
   "source": [
    "image_path = \"../image\"\n",
    "\n",
    "classes = []\n",
    "classCount = 20\n",
    "\n",
    "for root, dirs, classNames in os.walk(image_path):\n",
    "\tdirs.sort()\n",
    "\ti = 0\n",
    "\tfor dir_ in dirs:\n",
    "\t\tif i < classCount:\n",
    "\t\t\ti += 1\n",
    "\t\t\tclasses.append(os.path.basename(dir_))\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\n",
    "x = list()\n",
    "y_label = list()\n",
    "index = 0\n",
    "\n",
    "for className in classes:\n",
    "\tclass_path = os.path.join(data_path, className)\n",
    "\tfor root, _, fileNames in os.walk(class_path):\n",
    "\t\tfor fileName in fileNames:\n",
    "\t\t\tif fileName != '.DS_Store':\n",
    "\t\t\t\tdataItem = cv.imread(os.path.join(root, fileName), cv.IMREAD_GRAYSCALE)\n",
    "\t\t\t\ttransf = transforms.ToTensor()\n",
    "\t\t\t\timageTensor = transf(dataItem)\n",
    "\t\t\t\tif imageTensor.size()[2] != 148:\n",
    "\t\t\t\t\tprint(fileName)\n",
    "\t\t\t\tx.append(imageTensor)\n",
    "\t\t\t\ty_label.append([index, int(root[-4:])])\n",
    "\tindex += 1\n",
    "\n",
    "y_label = np.array(y_label)\n",
    "y = y_label[:, 0]\n",
    "\n",
    "print(y)\n",
    "print(len(x), len(y))\n",
    "print(x[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 1406\n",
      "x_test: 5626\n",
      "Shape of x_train: torch.Size([1, 369, 148])\n",
      "Shape of y_train: (1406,)\n",
      "Shape of x_test: torch.Size([1, 369, 148])\n",
      "Shape of y_test: (5626,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8)\n",
    "\n",
    "print(\"x_train:\", len(x_train))\n",
    "print(\"x_test:\", len(x_test))\n",
    "print(\"Shape of x_train:\", x_train[0].shape)\n",
    "print(\"Shape of y_train:\", np.shape(y_train))\n",
    "print(\"Shape of x_test:\", x_test[0].shape)\n",
    "print(\"Shape of y_test:\", np.shape(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1406, 1, 369, 148])\n",
      "torch.Size([5626, 1, 369, 148])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.stack(x_train, dim=0)\n",
    "x_test = torch.stack(x_test, dim=0)\n",
    "print(x_train.size())\n",
    "print(x_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "class TrainDatasets(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\tself.len = x_train.shape[0]\n",
    "\t\tself.X_train = x_train\n",
    "\t\tself.Y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.X_train[index], self.Y_train[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len\n",
    "\n",
    "class TestDatasets(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\tself.len = x_test.shape[0]\n",
    "\t\tself.X_test = x_test\n",
    "\t\tself.Y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.X_test[index], self.Y_test[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len\n",
    "\n",
    "train_dataset = TrainDatasets()\n",
    "test_dataset = TestDatasets()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(classCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_in_channel = 1\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size = x.shape[0]\n",
    "\t\treturn x.view(batch_size, -1)\n",
    "\n",
    "class SpecCNN(nn.Module):\n",
    "\tdef __init__(self, in_channel=_in_channel):\n",
    "\t\tsuper(SpecCNN, self).__init__()\n",
    "\t\tself.cnn = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 128, 50, stride=3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(128),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=5, stride=3),\n",
    "\t\t\tnn.Dropout(p=0.1),\n",
    "\n",
    "\t\t\tnn.Conv2d(128, 32, 3, stride=2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.BatchNorm2d(32),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tnn.Dropout(p=0.1),\n",
    "\n",
    "\t\t\tFlatten(),\n",
    "\t\t\tnn.Linear(in_features=512, out_features=256),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(p=0.1),\n",
    "\t\t\tnn.Linear(in_features=256, out_features=classCount)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x, ex_features=None):\n",
    "\t\treturn self.cnn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpecCNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(50, 50), stride=(3, 3))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=5, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.1, inplace=False)\n",
      "    (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (6): ReLU()\n",
      "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.1, inplace=False)\n",
      "    (10): Flatten()\n",
      "    (11): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Dropout(p=0.1, inplace=False)\n",
      "    (14): Linear(in_features=256, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SpecCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = nn.CrossEntropyLoss()\n",
    "optimi = optimizer.Adam(model.parameters(), lr=0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] Loss: 0.09836342\n",
      "[1,    20] Loss: 0.09784160\n",
      "[1,    30] Loss: 0.09280744\n",
      "[1,    40] Loss: 0.08603022\n",
      "[1,    50] Loss: 0.08132563\n",
      "[1,    60] Loss: 0.07601061\n",
      "[1,    70] Loss: 0.07267378\n",
      "Accuracy on test set: 40 %\n",
      "[2,    10] Loss: 0.06336585\n",
      "[2,    20] Loss: 0.05587473\n",
      "[2,    30] Loss: 0.05389811\n",
      "[2,    40] Loss: 0.04754989\n",
      "[2,    50] Loss: 0.04721261\n",
      "[2,    60] Loss: 0.04076739\n",
      "[2,    70] Loss: 0.03611719\n",
      "Accuracy on test set: 67 %\n",
      "[3,    10] Loss: 0.03136066\n",
      "[3,    20] Loss: 0.03040554\n",
      "[3,    30] Loss: 0.02653314\n",
      "[3,    40] Loss: 0.02501999\n",
      "[3,    50] Loss: 0.02922982\n",
      "[3,    60] Loss: 0.02525796\n",
      "[3,    70] Loss: 0.02331463\n",
      "Accuracy on test set: 75 %\n",
      "[4,    10] Loss: 0.02311069\n",
      "[4,    20] Loss: 0.01908012\n",
      "[4,    30] Loss: 0.01972912\n",
      "[4,    40] Loss: 0.02262194\n",
      "[4,    50] Loss: 0.01986765\n",
      "[4,    60] Loss: 0.01864532\n",
      "[4,    70] Loss: 0.02139099\n",
      "Accuracy on test set: 78 %\n",
      "[5,    10] Loss: 0.01265982\n",
      "[5,    20] Loss: 0.01356320\n",
      "[5,    30] Loss: 0.01384961\n",
      "[5,    40] Loss: 0.01566219\n",
      "[5,    50] Loss: 0.01404111\n",
      "[5,    60] Loss: 0.01631809\n",
      "[5,    70] Loss: 0.01429872\n",
      "Accuracy on test set: 81 %\n",
      "[6,    10] Loss: 0.01220161\n",
      "[6,    20] Loss: 0.01328069\n",
      "[6,    30] Loss: 0.01039522\n",
      "[6,    40] Loss: 0.01227979\n",
      "[6,    50] Loss: 0.01211124\n",
      "[6,    60] Loss: 0.01261451\n",
      "[6,    70] Loss: 0.01174494\n",
      "Accuracy on test set: 82 %\n",
      "[7,    10] Loss: 0.01054003\n",
      "[7,    20] Loss: 0.01264875\n",
      "[7,    30] Loss: 0.00781612\n",
      "[7,    40] Loss: 0.01180886\n",
      "[7,    50] Loss: 0.00804862\n",
      "[7,    60] Loss: 0.00975195\n",
      "[7,    70] Loss: 0.00744481\n",
      "Accuracy on test set: 86 %\n",
      "[8,    10] Loss: 0.00697500\n",
      "[8,    20] Loss: 0.00458945\n",
      "[8,    30] Loss: 0.00486753\n",
      "[8,    40] Loss: 0.00523537\n",
      "[8,    50] Loss: 0.00692098\n",
      "[8,    60] Loss: 0.00633787\n",
      "[8,    70] Loss: 0.00585765\n",
      "Accuracy on test set: 87 %\n",
      "[9,    10] Loss: 0.00539255\n",
      "[9,    20] Loss: 0.00560885\n",
      "[9,    30] Loss: 0.00474223\n",
      "[9,    40] Loss: 0.00415521\n",
      "[9,    50] Loss: 0.00487509\n",
      "[9,    60] Loss: 0.00507707\n",
      "[9,    70] Loss: 0.00554813\n",
      "Accuracy on test set: 85 %\n",
      "[10,    10] Loss: 0.00428016\n",
      "[10,    20] Loss: 0.00510569\n",
      "[10,    30] Loss: 0.00531315\n",
      "[10,    40] Loss: 0.00552165\n",
      "[10,    50] Loss: 0.00380107\n",
      "[10,    60] Loss: 0.00607302\n",
      "[10,    70] Loss: 0.00553949\n",
      "Accuracy on test set: 88 %\n",
      "[11,    10] Loss: 0.00302103\n",
      "[11,    20] Loss: 0.00471011\n",
      "[11,    30] Loss: 0.00486193\n",
      "[11,    40] Loss: 0.00427938\n",
      "[11,    50] Loss: 0.00376491\n",
      "[11,    60] Loss: 0.00329723\n",
      "[11,    70] Loss: 0.00294551\n",
      "Accuracy on test set: 89 %\n",
      "[12,    10] Loss: 0.00264667\n",
      "[12,    20] Loss: 0.00201691\n",
      "[12,    30] Loss: 0.00280362\n",
      "[12,    40] Loss: 0.00244381\n",
      "[12,    50] Loss: 0.00246496\n",
      "[12,    60] Loss: 0.00233193\n",
      "[12,    70] Loss: 0.00262802\n",
      "Accuracy on test set: 90 %\n",
      "[13,    10] Loss: 0.00188849\n",
      "[13,    20] Loss: 0.00215951\n",
      "[13,    30] Loss: 0.00180295\n",
      "[13,    40] Loss: 0.00189884\n",
      "[13,    50] Loss: 0.00178316\n",
      "[13,    60] Loss: 0.00241514\n",
      "[13,    70] Loss: 0.00158017\n",
      "Accuracy on test set: 91 %\n",
      "[14,    10] Loss: 0.00174997\n",
      "[14,    20] Loss: 0.00157089\n",
      "[14,    30] Loss: 0.00139587\n",
      "[14,    40] Loss: 0.00194567\n",
      "[14,    50] Loss: 0.00238059\n",
      "[14,    60] Loss: 0.00192728\n",
      "[14,    70] Loss: 0.00159983\n",
      "Accuracy on test set: 90 %\n",
      "[15,    10] Loss: 0.00141641\n",
      "[15,    20] Loss: 0.00115560\n",
      "[15,    30] Loss: 0.00115571\n",
      "[15,    40] Loss: 0.00100699\n",
      "[15,    50] Loss: 0.00109392\n",
      "[15,    60] Loss: 0.00130981\n",
      "[15,    70] Loss: 0.00112161\n",
      "Accuracy on test set: 91 %\n",
      "[16,    10] Loss: 0.00102926\n",
      "[16,    20] Loss: 0.00102065\n",
      "[16,    30] Loss: 0.00106487\n",
      "[16,    40] Loss: 0.00132205\n",
      "[16,    50] Loss: 0.00102546\n",
      "[16,    60] Loss: 0.00141832\n",
      "[16,    70] Loss: 0.00163083\n",
      "Accuracy on test set: 91 %\n",
      "[17,    10] Loss: 0.00108458\n",
      "[17,    20] Loss: 0.00084979\n",
      "[17,    30] Loss: 0.00113429\n",
      "[17,    40] Loss: 0.00071561\n",
      "[17,    50] Loss: 0.00079840\n",
      "[17,    60] Loss: 0.00058030\n",
      "[17,    70] Loss: 0.00049158\n",
      "Accuracy on test set: 92 %\n",
      "[18,    10] Loss: 0.00055436\n",
      "[18,    20] Loss: 0.00062840\n",
      "[18,    30] Loss: 0.00085751\n",
      "[18,    40] Loss: 0.00070692\n",
      "[18,    50] Loss: 0.00087090\n",
      "[18,    60] Loss: 0.00086712\n",
      "[18,    70] Loss: 0.00085273\n",
      "Accuracy on test set: 92 %\n",
      "[19,    10] Loss: 0.00059306\n",
      "[19,    20] Loss: 0.00041954\n",
      "[19,    30] Loss: 0.00061435\n",
      "[19,    40] Loss: 0.00052229\n",
      "[19,    50] Loss: 0.00065706\n",
      "[19,    60] Loss: 0.00059965\n",
      "[19,    70] Loss: 0.00037687\n",
      "Accuracy on test set: 92 %\n",
      "[20,    10] Loss: 0.00049436\n",
      "[20,    20] Loss: 0.00039680\n",
      "[20,    30] Loss: 0.00062719\n",
      "[20,    40] Loss: 0.00055129\n",
      "[20,    50] Loss: 0.00077305\n",
      "[20,    60] Loss: 0.00111877\n",
      "[20,    70] Loss: 0.00084015\n",
      "Accuracy on test set: 91 %\n"
     ]
    }
   ],
   "source": [
    "correct_list = []\n",
    "\n",
    "def train(epoch):\n",
    "\trunning_loss = 0.0\n",
    "\tfor batch_idx, data in enumerate(train_loader):\n",
    "\t\tinputs, target = data\n",
    "\t\toptimi.zero_grad()\n",
    "\t\toutputs = model(inputs)\n",
    "\t\tloss = Loss(outputs, target)\n",
    "\t\tloss.backward()\n",
    "\t\toptimi.step()\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif batch_idx % 10 == 9:\n",
    "\t\t\tprint('[%d, %5d] Loss: %.8f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "\t\t\trunning_loss = 0.0\n",
    "\n",
    "def test():\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data in test_loader:\n",
    "\t\t\timages, labels = data\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\t_, predicted = torch.max(outputs.data, dim=1)\n",
    "\t\t\ttotal += len(labels)\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "\t\tcorrect_list.append((100 * correct) / total)\n",
    "\t\tprint('Accuracy on test set: %d %%' % (100 * correct / total))\n",
    "\n",
    "for epoch in range(20):\n",
    "\ttrain(epoch)\n",
    "\ttest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_path = '../model'\n",
    "\n",
    "localtime = time.asctime(time.localtime(time.time()))\n",
    "pth_name = str(localtime) + '.pth'\n",
    "\n",
    "torch.save(model, os.path.join(model_path, pth_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f285020f8f5854762e6b0ffaf2f278742197f0f0ba066e81cd9123fd82c3c00"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
